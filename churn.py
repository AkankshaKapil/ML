# -*- coding: utf-8 -*-
"""Churn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBTU01IdeWxeewxHDqYnhkwC8wx-8LpJ
"""

# Commented out IPython magic to ensure Python compatibility.
#importing the library

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""DATA ANALYSIS"""

data = pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')

data.shape#this means data have 7043 rows and 21 columns

data #displaying the dataset

data.isna().sum()#checking for the missing values

#value_counts() function returns object containing counts of unique values. 
#The resulting object will be in descending order so that the first element is the most frequently-occurring element. 
#Excludes NA values by default
data.Churn.value_counts()

data.columns

#so this displays all the columns that have output like yes or no /0 and 1 or anything but only 2
columns=data.columns
binary_cols=[]
for col in columns:
  if data[col].value_counts().shape[0] == 2:
    binary_cols.append(col)
binary_cols

fig, axes = plt.subplots(2,3,sharey=True)
sns.countplot("gender",data=data,ax=axes[0,0])
sns.countplot("SeniorCitizen",data=data,ax=axes[0,1])
sns.countplot("Partner",data=data,ax=axes[0,2])
sns.countplot("Dependents",data=data,ax=axes[1,0])
sns.countplot("PhoneService",data=data,ax=axes[1,1])
sns.countplot("PaperlessBilling",data=data,ax=axes[1,2])

churn_numeric = {'Yes':1 , 'No':0}
data.Churn.replace(churn_numeric,inplace=True)

data[['gender','Churn']].groupby(['gender']).mean()

data[['SeniorCitizen','Churn']].groupby(['SeniorCitizen']).mean()

data[['Partner','Churn']].groupby(['Partner']).mean()

data[['Dependents','Churn']].groupby(['Dependents']).mean()

data[['PhoneService','Churn']].groupby(['PhoneService']).mean()

data[['PaperlessBilling','Churn']].groupby(['PaperlessBilling']).mean()

sns.countplot('InternetService',data=data)

sns.countplot('Contract',data=data)

fig, axes = plt.subplots(1,2, figsize=(12, 7))
sns.distplot(data["tenure"], ax=axes[0])
sns.distplot(data["MonthlyCharges"], ax=axes[1])

# Dropping these as they dont have much significance over churn
data.drop(['customerID','gender','PhoneService','Contract','TotalCharges'], axis=1, inplace=True)

data

"""DATA PREPROCESSING"""

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler

cat_features = ['SeniorCitizen', 'Partner', 'Dependents',
'MultipleLines', 'InternetService','OnlineSecurity','OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',
'StreamingMovies', 'PaperlessBilling', 'PaymentMethod']

Y = pd.get_dummies(data,columns=cat_features ,drop_first=True)

Y

sc = MinMaxScaler()
a = sc.fit_transform(data[['tenure']])
b = sc.fit_transform(data[['MonthlyCharges']])
Y['tenure'] = a
Y['MonthlyCharges'] = b
Y

sns.countplot('Churn', data=data).set_title('Before Resampling')

Y_no = Y[Y.Churn == 0]
Y_yes = Y[Y.Churn == 1]
Y_yes_upsampled = Y_yes.sample(n=len(Y_no), replace=True, random_state=42)
print(len(Y_yes),len(Y_yes_upsampled))

Y_upsampled = Y_no.append(Y_yes_upsampled).reset_index(drop=True)
sns.countplot('Churn', data=Y_upsampled).set_title('After Resampling')

Y_upsampled.Churn.value_counts()

from sklearn.model_selection import train_test_split
X = Y_upsampled.drop(['Churn'], axis=1)
y = Y_upsampled['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

from sklearn.linear_model import RidgeClassifier#The Ridge Classifier, based on Ridge regression method, converts the label data into [-1, 1] and solves the problem with regression method.
from sklearn.metrics import accuracy_score

clf_ridge = RidgeClassifier()
clf_ridge.fit(X_train, y_train)

pred = clf_ridge.predict(X_train)
accuracy_score(y_train, pred)

pred = clf_ridge.predict(X_test)
accuracy_score(y_test,pred)

"""Accuracy on training set is 75.74%
Accuracy in test set is 76.08%
"""

# Trying different model
from sklearn.ensemble import RandomForestClassifier
clf_forest = RandomForestClassifier(n_estimators=100, max_depth=10)
clf_forest.fit(X_train, y_train)

pred = clf_forest.predict(X_train)
accuracy_score(y_train, pred)

pred_test = clf_forest.predict(X_test)
accuracy_score(y_test, pred_test)

from sklearn import svm
clf_svm = svm.SVC()
clf_svm.fit(X, y)

pred = clf_svm.predict(X_train)
accuracy_score(y_train, pred)

pred_test = clf_svm.predict(X_test)
accuracy_score(y_test, pred_test)

from sklearn import tree
clf_tree = tree.DecisionTreeClassifier()
clf_tree.fit(X, y)

pred = clf_tree.predict(X_train)
accuracy_score(y_train, pred)

pred_test = clf_tree.predict(X_test)
accuracy_score(y_test, pred_test)

from sklearn.model_selection import GridSearchCV
parameters = {'n_estimators':[150,200,250,300], 'max_depth':[15,20,25]}
forest = RandomForestClassifier()
clf = GridSearchCV(estimator=forest, param_grid=parameters, n_jobs=-1, cv=5)

clf.fit(X, y)

print(clf.best_params_)
print(clf.best_score_)

"""As we can see Decision Tree Classifer gives the best accuracy .
We used grid search cv to increase accuracy of Random Forest and it incresed to 90%

MODELS USED:
1.Ridge Classifier
2.Random Forest
3.SVM
4.Decision Tree
5.Improved Random Forest using grid search cv

"""